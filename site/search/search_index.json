{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Peter Larsson's HPC notes","text":"<p>Pages with various notes on high-performance computing (HPC) related topics, especially AMD GPUs and software for electronic structure calculations and molecular dynamics.</p>"},{"location":"#opinion-disclaimer","title":"Opinion disclaimer","text":"<p>The content here are my own opinions and do not represent the views or opinions of LUMI or the KTH Royal Institute of Technology, or any of my future employers.</p>"},{"location":"#about-me","title":"About me","text":"<p>HPC application expert at the Parallel Computing Centre at Royal Institute of Technology in Sweden. I work in the LUMI User Support Team, helping scientists make the best out LUMI, one of the European pre-exascale supercomputers and currently the #3 supercomputer in the world. </p> <p></p> <p>Previously, I worked at Atos in Germany in the HPC division, and at the National Supercomputer Centre in Link\u00f6ping, Sweden. I have background in computational materials science, so you can find some information about scientific software for density functional theory (DFT) calculations here.</p>"},{"location":"#contact","title":"Contact","text":"<p>You can always reach me at: ypetla@kth.se, but please realize that I may not be able to respond to all requests for help with installing HPC applications due to time contraints.</p>"},{"location":"benchmarks/cp2k-lumi/","title":"CP2K Standard Benchmarks on LUMI","text":"<p>The CP2K program contains a set of standard benchmarks, which are distributed with code. You can find them in the <code>benchmarks/</code> directory in the source code folder. On the CP2K, you can find some data for other HPC clusters on the performance page.</p>"},{"location":"benchmarks/cp2k-lumi/#preliminary-lumi-g-benchmarks-feb-2023","title":"Preliminary LUMI-G benchmarks (Feb. 2023)","text":"<p>These were run in the beginning of 2023 with the CP2K 2023.1 release version compiled according to the instructions published in the howto section. On LUMI-C, no special core binding was used except what is provided by SLURM using e.g. <code>#SBATCH -c 8</code> for MPI + OpenMP jobs.</p>"},{"location":"benchmarks/cp2k-lumi/#qsh2o-x-standard-dft-molecular-dynamics","title":"QS/H2O-x (Standard DFT molecular dynamics)","text":"<p>These benchmark runs ab initio molecular dynamics (MD) with DFT, on a varying number of water molecules (from 32 to 8192) with TZV2P basis and plane-wave cutoff of 280 Ry. On LUMI-C, </p> <p></p> <p>The data points are the wall-time taken per MD steps (i.e. one fully converged SCF cycle, typically only a few SCF steps in MD simulations). On LUMI-C, it is possible to run huge cell with a relatively small number of compute nodes. The parallel scaling is decent, but could be better, when comparing to e.g. these benchmarks from Cray XC30 Archer in 2013. It is possible that it related to the network issues seen on LUMI at the start, which still persist.</p> <p>For smaller jobs (1-4 compute nodes), using 64 MPI ranks per node and <code>OMP_NUM_THREADS=2</code> is the best, but for larger jobs (8+ nodes) running with 32 ranks per node and <code>OMP_NUM_THREADS=8</code> tends to be faster.</p> <p>But can these be run on GPUs in the LUMI-G partition? The table below compared the time per MD step when running on LUMI-G nodes (8 GPUs/56 cores) vs LUMI-C nodes (128 cores):</p> Benchmark Nodes LUMI-C LUMI-G Speed-up H2O-256 1 6.6 s 14.2s 0.46x H2O-512 4 11.0 s 21.0s 0.52x H2O-1024 8 35.2 s 58.5s 0.60x <p>It turns out the that GPU acceleration on LUMI is not that effective for regular DFT calculations. The time taken per MD step is almost twice that of LUMI-C node when comparing 1 LUMI-G node vs 1 LUMI-C node. The results are somewhat disappointing, but a check in the output confirms that many operations are not GPU-accelerated, e.g.</p> <pre><code>-------------------------------------------------------------------------------\n-                                                                             -\n-                                DBCSR STATISTICS                             -\n-                                                                             -\n-------------------------------------------------------------------------------\nCOUNTER                                    TOTAL       BLAS       SMM       ACC\nflops     9 x     9 x    32        3698882869248       0.0%    100.0%      0.0%\nflops    32 x    32 x    32        3807488507904       0.0%      0.0%    100.0%\nflops    22 x     9 x    32        5135416602624       0.0%    100.0%      0.0%\nflops     9 x    22 x    32        5151420223488       0.0%    100.0%      0.0%\nflops    22 x    22 x    32        7121164328960       0.0%    100.0%      0.0%\nflops    32 x    32 x     9       10427106852864       0.0%    100.0%      0.0%\nflops    32 x    32 x    22       12744241709056       0.0%    100.0%      0.0%\nflops     9 x    32 x    32       12749107101696       0.0%    100.0%      0.0%\nflops    22 x    32 x    32       15582242013184       0.0%    100.0%      0.0%\nflops     9 x    32 x     9       30219541585920       0.0%    100.0%      0.0%\nflops    22 x    32 x     9       39766190137344       0.0%    100.0%      0.0%\nflops     9 x    32 x    22       39766190137344       0.0%    100.0%      0.0%\nflops    22 x    32 x    22       52085304426496       0.0%    100.0%      0.0%\nflops inhomo. stacks                           0       0.0%      0.0%      0.0%\nflops total                       238.254296E+12       0.0%     98.4%      1.6%\nflops max/rank                     29.798089E+12       0.0%     98.4%      1.6%\nmatmuls inhomo. stacks                         0       0.0%      0.0%      0.0%\nmatmuls total                        17486460992       0.0%     99.7%      0.3%\nnumber of processed stacks               2865104       0.0%     86.1%     13.9%\naverage stack size                                     0.0    7061.9     146.3\nmarketing flops                   375.300685E+12\n-------------------------------------------------------------------------------\n</code></pre> <p>Only a subset of operations (32x32x32) are running on the GPU (100% in the <code>ACC</code> column). It then makes sense that the runtime is doubled with half the number of CPU core per node.</p>"},{"location":"benchmarks/cp2k-lumi/#qs_dm_ls-linear-scaling-dft","title":"QS_DM_LS (linear scaling DFT)","text":"<p>What about linear scaling DFT? Does it work better on the GPU nodes. Using the previous benchmarks (see below) as a baseline, I get the following using 8 MPI ranks/8 GPUs per LUMI-G node and <code>OMP_NUM_THREADS=7</code> to account for only 63 available cores.</p> Nodes LUMI-C LUMI-G Speed-up 4 723 s 436 s 1.65x 8 462 s 278 s 1.65x <p>In this case, there is some speed-up, even if it is not dramatic. The ratio of GPU-node to CPU-node speed seems to be somewhat better (1.6x vs 1.3x) compared to what was observed on Piz Daint with GPU acceleration (this page). The output file confirms much more GPU usage, for example in the DBCSR library:</p> <pre><code>-------------------------------------------------------------------------------\n-                                                                             -\n-                                DBCSR STATISTICS                             -\n-                                                                             -\n-------------------------------------------------------------------------------\nCOUNTER                                    TOTAL       BLAS       SMM       ACC\nflops    23 x    23 x    23     2229574562881438       0.0%      0.0%    100.0%\nflops inhomo. stacks                           0       0.0%      0.0%      0.0%\nflops total                         2.229575E+15       0.0%      0.0%    100.0%\nflops max/rank                     79.250686E+12       0.0%      0.0%    100.0%\nmatmuls inhomo. stacks                         0       0.0%      0.0%      0.0%\nmatmuls total                        91623841657       0.0%      0.0%    100.0%\nnumber of processed stacks               3127188       0.0%      0.0%    100.0%\naverage stack size                                     0.0       0.0   29299.1\nmarketing flops                   891.964171E+15\n-------------------------------------------------------------------------------\n</code></pre> <p>It is doubtful, though, if it makes economic sense to run these kind of calculations on LUMI-G, as you would typically need to see a speed-up of at least 3x to 4x for it to make sense.</p> <p>An interesting find is that activating huge memory pages (2 MB) significantly speeds up the GPU version of the code (but not so much the CPU-only version). For example, the original timing for the 4 LUMI-G node run was ca 540 s without huge pages, compared to 436 s when the binary is linked with the <code>craype-hugepages2M</code> module. It is about 25% faster. Increasing the huge page size further to 8 MB did not result in any more improvement.</p>"},{"location":"benchmarks/cp2k-lumi/#qs_mp2_rpa-mp2-and-rpa-calculations","title":"QS_mp2_rpa (MP2 and RPA calculations)","text":"<p>Next up! These should be faster on GPUs...</p>"},{"location":"benchmarks/cp2k-lumi/#lumi-c-benchmarks-from-jan-2022","title":"LUMI-C benchmarks from Jan. 2022","text":"<p>These were run on LUMI-C during 2022 with CP2K 9.1 built from the LUMI EasyBuild recipes.</p>"},{"location":"benchmarks/cp2k-lumi/#qs_dm_ls-linear-scaling-dft_1","title":"QS_DM_LS Linear scaling DFT","text":"<p>This benchmark using the linear scaling DFT method in CP2K and simulates 6912 water molecules (21k atoms) with DZVP basis and 300 Ry cut-off. It runs for 2 SCF iterations (which is not enough to achieve convergence).</p> <p>I was able to ran some benchmarks with many nodes on LUMI-C during Jan 2022 with CP2K 9.1 and no extensive tuning other testing different values of <code>OMP_NUM_THREADS</code>, but the results were not impressive compared to what was published before for e.g. the Piz Daint and Noctua clusters on the CP2K web page:</p> <p></p> <p>I was not able to scale out sufficiently to beat the fastest time to solution as run previously on e.g. the Piz Daint (the year 2015 configuration with Cray XC30 with Intel Xeon Sandy Bridge and 1 Nvidia Tesla GPU K20X per node?) and the Noctua clusters (Intel Xeon Skylake with 100 Gbit/s Omni-Path network). It seems to primarily be a network/scaling problem as the speed on low core counts (256-512) is still ok.</p> <p>Running with 16 MPI ranks per compute node with 8 OpenMP threads (<code>OMP_NUM_THREADS=8</code>) was generally the fastest. I advise to always use at least <code>OMP_NUM_THREADS=2</code> when running on LUMI-C, as empirically, loading 128 MPI ranks per node increases the risk of running into various network problems.</p>"},{"location":"explanations/","title":"Explanatory pages","text":"<p>Pages describing certain topics and trying to explain how things work. These are aimed at learning and general understanding, together with the tutorials.</p>"},{"location":"howto/","title":"How-to guides","text":"<p>Guides on how to perform certain tasks like building and compiling software packages, and using certain HPC tools.</p>"},{"location":"howto/cp2k-lumi/","title":"How to compile CP2K 2023.1 with AMD GPU support","text":"<p>The CP2K software package is one the first to get AMD GPU support in many parts of package. Please see the GPU page in the documentation for an overview of which parts are accelerated. RPA calculations and linear scaling DFT were part of the delivery acceptance benchmark as part of the procurement of the LUMI and Dardel clusters.</p> <p>This guide is specifically for LUMI, but it should also apply to similar supercomputers like Frontier, Adastra, Setonix, and Dardel. It is based on the procedure described to me by Alfio Lazzaro at the HPE Center of Excellence for LUMI.</p>"},{"location":"howto/cp2k-lumi/#prerequisites","title":"Prerequisites","text":"<p>You need to have at least ROCm version 5.3, version 5.2 can compile the code, but it will not work properly (at least the DBCSR library) when you run it due to bugs in ROCm. You can check the version of ROCm available provided by the Cray programming environment with</p> <pre><code>$ module avail rocm\n\n-------------------- /users/peterlarsson/modules --------------------\nrocm/5.3.3\n\n------------------------ HPE-Cray PE modules ------------------------\nrocm/5.0.2 (D)\n\n---------------------- Non-PE HPE-Cray modules ----------------------\nrocm/5.0.2\n</code></pre> <p>Currently (Jan 2023), only ROCm 5.0 is available in LUMI, which means that you have to rely on an unsupported version of ROCm installed by the LUMI support team. You can see this extra ROCm module by activating the <code>CrayEnv</code> module:</p> <pre><code>module load CrayEnv rocm/5.3.3\n</code></pre> <p>I have used it successfully to run CP2K on the LUMI-G partition.</p>"},{"location":"howto/cp2k-lumi/#compile-environment","title":"Compile environment","text":"<p>Let us use Cray's GCC and FFTW libraries to compile CP2K. Starting from the default login environment (i.e. no <code>LUMI</code> or <code>spack</code> modules loaded):</p> <pre><code>module load CrayEnv\nmodule load PrgEnv-gnu/8.3.3\nmodule load cray-fftw/3.3.10.1\nmodule load craype-accel-amd-gfx90a\nmodule load rocm/5.3.3\n</code></pre> <p>In some cases, you might need to load a module with a more recent version of <code>cmake</code>, but in this case, it still works with the system default cmake (3.17.0).</p>"},{"location":"howto/cp2k-lumi/#step-1-cosma-with-gpu-aware-mpi-support","title":"Step 1: COSMA with GPU-aware MPI support","text":"<p>It is necessary to compile the COSMA library separately, in order to active GPU-aware MPI support in the library, as this is currently not done automatically in the toolchain install scripts for CP2K. This speeds up RPA calculations.</p> <pre><code>git clone --recursive https://github.com/eth-cscs/COSMA COSMA-2.6.2\ncd COSMA-2.6.2\nmkdir build\ncd build\nmkdir install\ncmake -DCMAKE_INSTALL_PREFIX=${PWD}/install -DCOSMA_BLAS=ROCM -DCOSMA_SCALAPACK=CRAY_LIBSCI -DCOSMA_WITH_TESTS=NO -DCOSMA_WITH_BENCHMARKS=NO -DCMAKE_CXX_COMPILER=CC -DCOSMA_WITH_APPS=NO -DCOSMA_WITH_PROFILING=NO -DBUILD_SHARED_LIBS=NO -DCOSMA_WITH_GPU_AWARE_MPI=ON ..    \nmake\nmake install\n</code></pre> <p>The special configuration flag here is <code>-DCOSMA_WITH_GPU_AWARE_MPI=ON</code>. The build should be fast, even in serial mode, a few minutes maximum. If you check the <code>install/</code> directory, you should see the following:</p> <pre><code>$ ls install/lib64/\ncmake  libcosma.a  libcosma_prefixed_pxgemm.a  libcosma_pxgemm_cpp.a  libcosta.a  libcosta_prefixed_scalapack.a  libcosta_scalapack.a  libTiled-MM.a  pkgconfig\n</code></pre> <p>These COSMA libraries will later be used by the CP2K install script. Unfortunately, the install script expects to find the libraries in <code>/lib</code> and not <code>/lib64</code>. We can fix this by just making a symbolic link.</p> <pre><code>$ cd install\n$ ln -s lib64 lib\n</code></pre> <p>We are now ready to starting installing CP2K.</p>"},{"location":"howto/cp2k-lumi/#step-2-cp2k-release-version-20231","title":"Step 2: CP2K release version 2023.1","text":"<p>Checkout the latest CP2K release. In January 2023, this was \"2023.1\", which contains AMD GPU support.</p> <pre><code>mkdir cp2k\ncd cp2k\ngit clone --recursive https://github.com/cp2k/cp2k.git 2023.1\ncd 2023.1\ngit checkout v2023.1\n</code></pre> <p>The first step is to build some libraries used by CP2K. This can be done with the provided <code>install_cp2k_toolchain</code> script:</p> <pre><code>cd tools/toolchain\n./install_cp2k_toolchain.sh -j 16 --enable-cray --enable-hip=yes --gpu-ver=Mi250 --with-cosma=/[path-to-cosma-above]/COSMA-2.6.2/build/install\n</code></pre> <p>The scripts provides several flags which are useful here: <code>enable-cray</code> to detect the Cray programming environment, <code>--enable-hip</code> for GPU support, but with AMD GPUs (HIP is like CUDA, but for AMD GPUs), and <code>--gpu-ver=Mi250</code> to select the GPU model corresponding to what is installed in LUMI, AMD MI250x. The output should begin like this:</p> <pre><code>WARNING: (./install_cp2k_toolchain.sh, line 330) No MPI installation detected (ignore this message in Cray Linux Environment or when MPI installation was requested).\n------------------------------------------------------------------------\nCRAY Linux Environment (CLE) is detected\n------------------------------------------------------------------------\npath to cc is /opt/cray/pe/craype/2.7.17/bin/cc\npath to ftn is /opt/cray/pe/craype/2.7.17/bin/ftn\npath to CC is /opt/cray/pe/craype/2.7.17/bin/CC\nFound include directory /opt/cray/pe/mpich/8.1.18/ofi/gnu/9.1/include\nFound include directory /opt/cray/pe/mpich/8.1.18/ofi/gnu/9.1/lib\nlibz is found in ld search path\nlibdl is found in ld search path\nCompiling with 16 processes for target native.\n</code></pre> <p>Building all the dependencies can take a considerable amount of time, for example, <code>libint</code> alone can take 1 hour, even using many cores. Compiling everyting with 16 cores (<code>-j 16</code> above) took ca 40 minutes. In the end of the terminal output, there will be some important information. You should see several lines like this indicating that ROCm has been found:</p> <pre><code>...\nlibhipblas is found in ld search path\nFound lib directory /pfs/lustrep1/projappl/project_4650000xx/rocm/rocm-5.3.3/lib\n...\n</code></pre> <p>And an instruction to copy the \"ARCH\" files with compile settings to the base directory</p> <pre><code>Now copy:\ncp /projappl/project_4650000xx/build/cp2k/2023.1/tools/toolchain/install/arch/* to the cp2k/arch/ directory\nTo use the installed tools and libraries and cp2k version\ncompiled with it you will first need to execute at the prompt:\nsource /projappl/project_4650000xx/build/cp2k/2023.1/tools/toolchain/install/setup\nTo build CP2K you should change directory:\ncd cp2k/\nmake -j 16 ARCH=local VERSION=\"ssmp sdbg psmp pdbg\"\n</code></pre> <p>After having copied the files to the <code>arch/</code> directory, we can go back to the CP2K base directory and compile the program:</p> <pre><code>cd ../..\nmake -j 16 ARCH=local_hip VERSION=\"psmp\"\n</code></pre> <p>Please note that the ARCH file called <code>local-hip</code> should be used, not the one named just <code>local</code> as written in the terminal output above! Compiling CP2K itself should be faster. I recorded about 7 minutes. The binaries are found in then <code>exe/local_hip/</code> folder:</p> <pre><code>$ ls exe/local_hip/\ncp2k.popt                         dbt_tas_unittest.psmp             grid_miniapp.psmp                 nequip_unittest.psmp\ncp2k.psmp                         dbt_unittest.psmp                 grid_unittest.psmp                parallel_rng_types_unittest.psmp\ncp2k_shell.psmp                   dumpdcd.psmp                      libcp2k_unittest.psmp             xyz2dcd.psmp\ndbm_miniapp.psmp                  graph.psmp                        memory_utilities_unittest.psmp\n</code></pre> <p>A basic sanity check is to see if some HIP libraries were included in the binary at link time (they should be):</p> <pre><code>$ ldd exe/local_hip/cp2k.psmp | grep hip\n    libamdhip64.so.5 =&gt; /projappl/project_4625000xx/rocm/rocm-5.3.3/lib/libamdhip64.so.5 (0x00007fe4e6c2d000)\n    libhipfft.so =&gt; /projappl/project_4650000xx/rocm/rocm-5.3.3/lib/libhipfft.so (0x00007fe4e99c8000)\n    libhipblas.so.0 =&gt; /projappl/project_4650000xx/rocm/rocm-5.3.3/lib/libhipblas.so.0 (0x00007fe4e69cb000)\n</code></pre>"},{"location":"howto/cp2k-lumi/#step-3-test-running-on-the-gpu-nodes","title":"Step 3: Test running on the GPU nodes","text":"<p>Here we will run the linear-scaling DFT benchmark provided with CP2K. It is a large simulation with 20,736 atoms. The linear scaling method uses the DBCSR matrix-matrix multiplication library, which uses GPUs. The benchmark located in the <code>benchmarks/QS_DM_LS/</code> folder. The recommended way to run is to use 8 MPI ranks per compute node (1 rank per GPU die), and 8 OpenMP threads per rank. As of January 2023, the so-called \"low noise\" mode is enabled on the compute nodes, which means that only 63 cores are available and the first core is reserved for the operating system. This requries some special CPU binding for best performance, and it also means that practically, it is easiest to run with 7 threads per rank (<code>OMP_NUM_THREADS=7</code>).</p> <p>The job script:</p> <pre><code>#!/bin/bash\n#SBATCH -J lsdft\n#SBATCH -p standard-g\n#SBATCH -A project_465000XXX\n#SBATCH --time=00:30:00\n#SBATCH --nodes=4\n#SBATCH --gres=gpu:8\n#SBATCH --exclusive\n#SBATCH --ntasks-per-node=8\n#SBATCH --cpus-per-task=7\n\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\n\nulimit -s unlimited\nexport OMP_STACKSIZE=512M\n\nexport MPICH_OFI_NIC_POLICY=GPU\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nmodule load rocm/5.3.3\n\nCP2K=/path/to/cp2k/2023.1/exe/local_hip/cp2k.psmp\nsrun --cpu-bind=mask_cpu:0xfe,0xfe00,0xfe0000,0xfe000000,0xfe00000000,0xfe0000000000,0xfe000000000000,0xfe00000000000000 ./select_gpu.sh ${CP2K} -i H2O-dft-ls.inp -o out-4n8r7t8g.1\n</code></pre> <p>The <code>export MPICH_GPU_SUPPORT_ENABLED=1</code> is not strictly necessary, since it is the default value, but I keep it there for reference.</p> <p>The <code>select_gpu.sh</code> helper script is useful to get the GPU to CPU binding correct on LUMI.</p> <pre><code>$ cat select_gpu.sh \n#!/bin/bash\n\nexport ROCR_VISIBLE_DEVICES=$SLURM_LOCALID\n#export ROCR_VISIBLE_DEVICES=0,1\n\nif [[ \"$SLURM_LOCALID\" == \"0\" ]]; then\nROCR_VISIBLE_DEVICES=4\nfi\n\nif [[ \"$SLURM_LOCALID\" == \"1\" ]]; then\nROCR_VISIBLE_DEVICES=5\nfi\n\nif [[ \"$SLURM_LOCALID\" == \"2\" ]]; then\nROCR_VISIBLE_DEVICES=2\nfi\n\nif [[ \"$SLURM_LOCALID\" == \"3\" ]]; then\nROCR_VISIBLE_DEVICES=3\nfi\n\nif [[ \"$SLURM_LOCALID\" == \"4\" ]]; then\nROCR_VISIBLE_DEVICES=6\nfi\n\nif [[ \"$SLURM_LOCALID\" == \"5\" ]]; then\nROCR_VISIBLE_DEVICES=7\nfi\n\nif [[ \"$SLURM_LOCALID\" == \"6\" ]]; then\nROCR_VISIBLE_DEVICES=0\nfi\n\nif [[ \"$SLURM_LOCALID\" == \"7\" ]]; then\nROCR_VISIBLE_DEVICES=1\nfi\n\necho \"Node: \" $SLURM_NODEID \"Local task id:\" $SLURM_LOCALID \"ROCR_VISIBLE_DEVICES\" $ROCR_VISIBLE_DEVICES\nexec $*\n</code></pre> <p>This script is useful for many applications using GPU on LUMI, not only CP2K.</p> <p>When I run this job, I get a runtime of 537 (+/-1) seconds on 4 LUMI-G nodes (with 32 GPUs in total). This can be compared with ca 723 (+/-1) seconds on 4 LUMI-C nodes (using all 128 cores and <code>OMP_NUM_THREADS=8</code>). The GPU speed-up is not dramatic for linear-scaling DFT, and may not be cost-effective, but it is a good start. It is possible to improve the speed a bit with 2 MB huge memory pages (436 s), but this can also slow down some other kinds of calculations, so it is not foolproof. I expect, though, that some other methods, like RPA calculations, will show much better performance on GPUs.</p> <p>I show some more benchmarks for CP2K on my CP2K benchmark page.</p>"},{"location":"reference/","title":"Reference pages","text":"<p>These pages contain random notes, mostly of reference character. I keep useful observations, snippets of code, links to documentation etc here. I also put benchmark results here.</p>"},{"location":"reference/mi250x/","title":"Technical information about the MI250x GPU","text":""},{"location":"reference/mi250x/#raw-floating-point-capabilities","title":"Raw floating point capabilities","text":"<p>This table compares the raw technical specifications of one GPU die (\"GCD\") in the MI250x GPU package with some Nvidia GPUs. The GCD is what appears as 1 GPU when you run on the compute node, so this is what makes most sense to compare with when doing benchmarking. Please note that AMD, in its marketing, often refers to the MI250x package with 2 GCDs as \"1 GPU\", which might be misleading. </p> Spec V100 A100 MI250x (1 GCD) Vector FP64 (TF) 7.8 9.7 23.9 Vector FP32 (TF) 15.7 19.5 23.9 Vector FP16 (TF) 31.4 78.0 191.5 Matrix FP64 (TF) 7.8 19.5 47.9 Matrix FP32 (TF) 15.7 156.0 47.9 Matrix FP16 (TF) 125.0 312.0 191.5 Matrix BF16 (TF) 125.0 312.0 191.5 Memory bandwidth (TB/s) 1.1 2.0 1.6 <p>These numbers give a rough estimation of what kind of performance you can expect (or should aim for) when porting Nvidia CUDA applications to AMD GPUs:</p> <ul> <li>If the code is limited by memory bandwidth you are likely to see the same or somewhat lower performance per GPU, if you compare 1 GCD to 1 A100.</li> <li>If the code uses FP64 math, like many traditional scientific simulations, there is a potential for 2x speedup vs 1 A100, but you may struggle to realize this performance due to the available memory bandwidth.</li> <li>If the code uses FP32 math, the performance should be about even, +/- 25% or so.</li> <li>If the code uses low-precision matrix operations (e.g. machine learning), the performance is likely to be lower per GPU.</li> </ul> <p>All in all, I consider 1 MI250x GCD to be roughly equal to 1 A100, as an easy rule of thumb. If you can get results like that, you are probably in a good spot already with respect to optimization.</p> <p>Sources:</p> <ul> <li>AMD's MI250x data sheet</li> <li>Nvidia's blog post on the Hopper architecture</li> <li>Nvidia's blog post on the Ampere architecture</li> </ul>"},{"location":"reference/rocm/","title":"ROCm","text":"<p>ROCM is similar to Nvidia's CUDA toolkit, but for AMD's GPUs.</p> <ul> <li>The most recent release notes are found in the docs.amd.com site.</li> <li>A translation table between CUDA and HIP calls (with a list of what is support and in which ROCm version): HIP Supported CUDA API Guide v5.4. Newer versions should become available with new ROCm releases.</li> </ul>"},{"location":"reference/rocm/#rocm-driver-compatibility","title":"ROCm driver compatibility","text":"<p>It is possible to use newer versions of the ROCm libraries and the HIP compiler together with older AMD GPU drivers in the kernel. This is useful as you cannot update the GPU drivers yourself on a HPC system without root access. Generally speaking, the ROCm userspace software tends to work with +/- 2 release of the kernel space software. The exact tested compatibility is not stated in the ROCm release notes, but you can find the compability table in the ROCm Deep Learning Guide under \"Prerequisites\". For example, with the ROCm 5.1.3 drivers on the compute nodes (which you may not be able to change as user), you can still use ROCm 5.4.0 userland utilities to compile and run your own programs.</p>"},{"location":"reference/rocm/#installing-rocm-in-a-custom-location","title":"Installing ROCm in a custom location","text":"<p>ROCm is normally installed in <code>/opt/rocm</code>. If, for various reasons, you would like to install several versions in your home directory, some modifications are typically necessary. At least for versions up to 5.3 (possibly latter), ROCm contains many hard-coded paths to <code>/opt/rocm/</code>, which it also inserts into compiled binaries, for example for generation just-in-time code in MIOpen. This creates some problems when using your own ROCm on a HPC system.</p>"},{"location":"reference/spack/","title":"Useful Spack command and operations","text":"<p>Spack is a package manager for HPC which makes it easy to install your own independent, and self consistent software stack, sometimes with many verions of the same library.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In the future, I would like to publish some tutorials and learning materials here.</p>"}]}